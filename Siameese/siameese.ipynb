{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc0188f",
   "metadata": {},
   "source": [
    "# Face verification app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee8747",
   "metadata": {},
   "source": [
    "We build an app that will be able to verify the user's face by distinguishing it from other faces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401a8d2",
   "metadata": {},
   "source": [
    "## Data dollection\n",
    "\n",
    "Build a dataset by using local camera and taking snapshots. We need positive pairs of pictures (where both pictures belong to the same face) and negative pairs (for this, we have used the Labelled Faces in the Wild dataset with also some different faces from camera, and some frames wit no faces).\n",
    "\n",
    "For this reason we create three directories, the first is the Anchor (first images in the pairs), the second is the Positive (second images in the pairs that match the anchors) and the third is the Negative directory (third images in the pairs that differentiate from the anchors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e431c",
   "metadata": {},
   "source": [
    "__Import libraries:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a791c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3783d68",
   "metadata": {},
   "source": [
    "__Verify GPU configured with Tensorflow:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6065c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d23c6c",
   "metadata": {},
   "source": [
    "__Create directories needed for our data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a099231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Paths\n",
    "POS_PATH = os.path.join('data','positive')\n",
    "NEG_PATH = os.path.join('data','negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')\n",
    "VER_PATH = os.path.join('application_data', 'verification_images')\n",
    "INP_PATH = os.path.join('application_data', 'input_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f94bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)\n",
    "os.makedirs(VER_PATH)\n",
    "os.makedirs(INP_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd527d0",
   "metadata": {},
   "source": [
    "__Uncompress the Labelled Faces in the Wild dataset and move them to /data/negative path:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1c0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move LFW images to data/negative\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36b39c",
   "metadata": {},
   "source": [
    "__Use OpenCV to establish connection to the webcam and take pictures. The frame must be cropped to be 250x250. Keyboard actions specified in order to save the pictures in the corresponding folders and break:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f6ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Resize frame to 250x250 px\n",
    "    frame = frame[190:190+250, 230:230+250, :]\n",
    "    \n",
    "    # Collect anchors\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)     \n",
    "\n",
    "    # Collect verifications\n",
    "    if cv2.waitKey(1) & 0XFF == ord('v'):\n",
    "        imgname = os.path.join(VER_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    # Breaking gracefully\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam        \n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae31277",
   "metadata": {},
   "source": [
    "additionaly, after some model tests we decide to also captured some more negative samples.\n",
    "\n",
    "a)photos from camera with no face at all (camera covered, wall )\n",
    "b)photos from camera with other faces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f3623",
   "metadata": {},
   "source": [
    "__Helper function to insert random noise (brightness, contrast, flip, jpeg_quality, saturation) to pictures that have been captured by the webcam, so that more samples are created and the model is able to generalize better:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674827ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(img):\n",
    "    data = []\n",
    "    for i in range(9):\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100), np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_saturation(img, lower=0.9, upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        \n",
    "        data.append(img)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0e7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_data_augmentation(file_path):\n",
    "    for file_name in os.listdir(os.path.join(file_path)):\n",
    "        img_path = os.path.join(file_path, file_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        augmented_images = data_augmentation(img)\n",
    "\n",
    "        for image in augmented_images:\n",
    "            cv2.imwrite(os.path.join(file_path, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f94ac",
   "metadata": {},
   "source": [
    "__Apply the data augmentation function to the whole Anchor and Positive directories:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05970af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_data_augmentation(ANC_PATH)\n",
    "apply_data_augmentation(POS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f0c76",
   "metadata": {},
   "source": [
    "__Take 580 sample file paths in a random shuffled manner from each directory:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b31d7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'\\*.jpg').take(580)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'\\*.jpg').take(580)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'\\*.jpg').take(580)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3309508",
   "metadata": {},
   "source": [
    "__Define function to load images from filepaths, decode jpegs, scale pixel values to facilitate training and resize images to 105x105 (input dimension for our conv layer):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864fccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (105,105))\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ba813",
   "metadata": {},
   "source": [
    "__Create labeled dataset with pairs of positives, negatives and labels (1,0)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5df153",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor,positive,tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor,negative,tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07eb3d1",
   "metadata": {},
   "source": [
    "__function to preprocess the sample pairs:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e33feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799939e",
   "metadata": {},
   "source": [
    "__Build dataloader pipeline to apply the preprocessing process, cache the dataset and shuffle in order to have both positive and negative pairs in train and test sets:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03511cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d23dc8",
   "metadata": {},
   "source": [
    "__Split dataset to train and test at 0.7, create batches of 16 and prefetch to improve latency and throughput:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f78266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.take(round(len(data)*0.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33f04cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.skip(round(len(data)*0.7))\n",
    "test_data = test_data.take(round(len(data)*0.3))\n",
    "test_data = test_data.batch(16, drop_remainder =True)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369abc6",
   "metadata": {},
   "source": [
    "__Build the embedding layer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714b1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inp = Input(shape=(105,105,3), name='input_image')\n",
    "    \n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ace2bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "703e331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 105, 105, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 96, 96, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 42, 42, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 21, 21, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 18, 18, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f1e74",
   "metadata": {},
   "source": [
    "__Define class of custom layer for L1 Distance between the embeddings:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ceed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7dd750",
   "metadata": {},
   "source": [
    "__Build the final siamese network model so as to process the pairs of images, calculate the L1 distance and do the binary classification:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebca73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "    \n",
    "    # Handle inputs\n",
    "    input_image = Input(name='input_img', shape=(105,105,3))\n",
    "    validation_image = Input(name='validation_img', shape=(105,105,3))\n",
    "    \n",
    "    \n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification Layer\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='siamese_network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b5537cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a1696",
   "metadata": {},
   "source": [
    "__Define Loss and optimizer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585de26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1922fb",
   "metadata": {},
   "source": [
    "__Create checkpoints for model training:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c5d252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c42aae",
   "metadata": {},
   "source": [
    "__Define the training step process, by using the GradientTape.gradient and the apply_gradients(). Convert it to a graph representation for better performance:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f6f2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        X = batch[:2]\n",
    "        y = batch[2]\n",
    "        \n",
    "        yhat = siamese_model(X, training=True)\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "            \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f79c1",
   "metadata": {},
   "source": [
    "__Training process to loop through the batches, perform the training step and accumulate statistics for the precision and recall for each epoch:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f61792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "    # loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch,EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            loss = train_step(batch)\n",
    "            yhat = siamese_model.predict(batch[:2], verbose=0)\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat)\n",
    "            progbar.update(idx+1)\n",
    "        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00dd40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d12cbc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/20\n",
      " 2/51 [>.............................] - ETA: 16:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947af1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue the model training on google colab due lack of gpu, check colab.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadfc59",
   "metadata": {},
   "source": [
    "__Evaluate model on the test set:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aff93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
    "   # yhat = siamese_model.predict([test_input, test_val], verbose=0)\n",
    "    yhat = model.predict([test_input, test_val], verbose=0)\n",
    "    r.update_state(y_true, yhat)\n",
    "    p.update_state(y_true, yhat)\n",
    "\n",
    "print(r.result().numpy(), p.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c14852",
   "metadata": {},
   "source": [
    "__Visualize the data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b3f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[8])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[8])\n",
    "plt.show()\n",
    "print(y_true[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d54a0",
   "metadata": {},
   "source": [
    "__Save the final model:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af070791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "siamese_model.save('siamesemodel_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e43fb203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('siamesemodel_v3.h5', custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e4584f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.3862670e-07],\n",
       "       [6.6421023e-08],\n",
       "       [5.6948692e-01],\n",
       "       [3.5669652e-07],\n",
       "       [9.9996823e-01],\n",
       "       [9.4013929e-01],\n",
       "       [2.7302539e-07],\n",
       "       [2.4127795e-07],\n",
       "       [3.6209050e-07],\n",
       "       [3.4748933e-01],\n",
       "       [9.9994469e-01],\n",
       "       [1.5882508e-06],\n",
       "       [2.4431767e-01],\n",
       "       [8.4498225e-08],\n",
       "       [9.9999410e-01],\n",
       "       [9.9725109e-01]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e848be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siamese_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 105, 105, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validaption_img (InputLayer)   [(None, 105, 105, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
      "                                                                  'validaption_img[0][0]']        \n",
      "                                                                                                  \n",
      " l1_dist_1 (L1Dist)             (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['l1_dist_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0deb81f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Plot the model structure\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e0686",
   "metadata": {},
   "source": [
    "__Verification function - Compare the input image with each one of the verification images. If the number of images that have been predicted as positive (p>detection_threshold) is greater than the verification_threshold, then the person is verified:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eb3a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "        input_img = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join('application_data', 'verification_images', image))\n",
    "        \n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "       \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images')))\n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afafb61",
   "metadata": {},
   "source": [
    "__Use the webcam to capture a photo as an input for the verification:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "419088c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 244ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 244ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "True\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 229ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 245ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 213ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[190:190+250, 230:230+250, :]\n",
    "    \n",
    "    cv2.imshow('Verification', frame)\n",
    "    \n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "    \n",
    "        results, verified = verify(model, 0.5, 0.5)\n",
    "        print(verified)\n",
    "        \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55021e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
